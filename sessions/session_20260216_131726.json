{
  "id": "session_20260216_131726",
  "task": "implement a thread-safe LRU cache with TTL expiry, async-compatible get/set, and hit/miss statistics tracking",
  "started_at": "2026-02-16T13:17:26.318475",
  "completed_at": "2026-02-16T13:32:21.540783",
  "config": {
    "creator": "claude",
    "reviewer": "codex",
    "critic": "gemini",
    "iterations": 2
  },
  "workspace_path": "/Users/apple/multi-code-agent/sessions/session_20260216_131726/workspace",
  "initial_code": "\"\"\"\nThread-safe LRU Cache with TTL Expiry and Async Support.\n\nProvides a doubly-linked list + hash map LRU eviction policy,\nper-entry TTL expiration, thread safety via RLock, async-compatible\nget/set methods (async wrappers around the sync core), and\nhit/miss statistics tracking.\n\"\"\"\n\nimport asyncio\nimport threading\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass, field\nfrom typing import Any, Generic, Hashable, Optional, TypeVar\n\nK = TypeVar(\"K\", bound=Hashable)\nV = TypeVar(\"V\")\n\n\n@dataclass\nclass CacheStats:\n    \"\"\"Snapshot of cache hit/miss counters.\"\"\"\n    hits: int = 0\n    misses: int = 0\n    evictions: int = 0\n    expired: int = 0\n\n    @property\n    def total(self) -> int:\n        return self.hits + self.misses\n\n    @property\n    def hit_rate(self) -> float:\n        \"\"\"Fraction of lookups that were cache hits (0.0 if no lookups).\"\"\"\n        return self.hits / self.total if self.total > 0 else 0.0\n\n    def __repr__(self) -> str:\n        return (\n            f\"CacheStats(hits={self.hits}, misses={self.misses}, \"\n            f\"evictions={self.evictions}, expired={self.expired}, \"\n            f\"hit_rate={self.hit_rate:.2%})\"\n        )\n\n\n@dataclass\nclass _Entry:\n    \"\"\"Internal cache entry storing value, expiry timestamp, and access order.\"\"\"\n    value: Any\n    # Absolute expiry time in seconds since epoch; None means no expiry.\n    expires_at: Optional[float]\n\n    def is_expired(self) -> bool:\n        if self.expires_at is None:\n            return False\n        return time.monotonic() >= self.expires_at\n\n\nclass LRUCache(Generic[K, V]):\n    \"\"\"\n    Thread-safe Least-Recently-Used (LRU) cache with optional per-entry TTL.\n\n    Parameters\n    ----------\n    capacity : int\n        Maximum number of entries. Must be >= 1.\n    default_ttl : float | None\n        Default time-to-live in seconds for entries that do not specify one.\n        ``None`` (default) means entries never expire unless explicitly set.\n\n    Thread safety\n    -------------\n    All public synchronous methods are protected by a reentrant lock so the\n    cache can be shared across threads without external synchronisation.\n\n    Async support\n    -------------\n    ``aget`` and ``aset`` are thin async wrappers that call the synchronous\n    core on the running event loop's thread-pool executor, making them safe\n    to ``await`` from async code without blocking the event loop.\n\n    Examples\n    --------\n    >>> cache: LRUCache[str, int] = LRUCache(capacity=3, default_ttl=60)\n    >>> cache.set(\"a\", 1)\n    >>> cache.get(\"a\")\n    1\n    >>> cache.get(\"missing\")  # returns None\n    >>> cache.stats()\n    CacheStats(hits=1, misses=1, ...)\n    \"\"\"\n\n    def __init__(self, capacity: int, default_ttl: Optional[float] = None) -> None:\n        if capacity < 1:\n            raise ValueError(f\"capacity must be >= 1, got {capacity}\")\n        if default_ttl is not None and default_ttl <= 0:\n            raise ValueError(f\"default_ttl must be > 0, got {default_ttl}\")\n\n        self._capacity = capacity\n        self._default_ttl = default_ttl\n        # OrderedDict preserves insertion/move order; last = most recently used.\n        self._cache: OrderedDict[K, _Entry] = OrderedDict()\n        self._lock = threading.RLock()\n\n        # Statistics counters (protected by the same lock)\n        self._hits = 0\n        self._misses = 0\n        self._evictions = 0\n        self._expired = 0\n\n    # ------------------------------------------------------------------\n    # Public synchronous API\n    # ------------------------------------------------------------------\n\n    def get(self, key: K, default: Optional[V] = None) -> Optional[V]:\n        \"\"\"\n        Retrieve *key* from the cache.\n\n        Returns *default* (``None`` by default) on cache miss or if the\n        entry has expired. Expired entries are lazily removed on access.\n\n        Updates the LRU order on hit so *key* becomes the most-recently used.\n        \"\"\"\n        with self._lock:\n            entry = self._cache.get(key)\n            if entry is None:\n                self._misses += 1\n                return default\n\n            if entry.is_expired():\n                # Lazy expiry: remove stale entry and count as miss.\n                del self._cache[key]\n                self._expired += 1\n                self._misses += 1\n                return default\n\n            # Move to end (most recently used position).\n            self._cache.move_to_end(key)\n            self._hits += 1\n            return entry.value  # type: ignore[return-value]\n\n    def set(self, key: K, value: V, ttl: Optional[float] = None) -> None:\n        \"\"\"\n        Insert or update *key* -> *value*.\n\n        Parameters\n        ----------\n        key : K\n            Cache key (must be hashable).\n        value : V\n            Value to store.\n        ttl : float | None\n            Per-entry TTL in seconds. Overrides ``default_ttl`` when given.\n            Pass ``0`` or a negative number to get an immediate expiry\n            (the entry will be expired on the next access).\n            ``None`` falls back to ``default_ttl``; if that is also ``None``\n            the entry never expires.\n        \"\"\"\n        with self._lock:\n            effective_ttl = ttl if ttl is not None else self._default_ttl\n            if effective_ttl is not None:\n                expires_at = time.monotonic() + effective_ttl\n            else:\n                expires_at = None\n\n            if key in self._cache:\n                # Update existing entry and promote to MRU position.\n                self._cache[key] = _Entry(value=value, expires_at=expires_at)\n                self._cache.move_to_end(key)\n            else:\n                # Evict LRU entry if at capacity.\n                if len(self._cache) >= self._capacity:\n                    self._cache.popitem(last=False)  # Remove oldest (LRU)\n                    self._evictions += 1\n                self._cache[key] = _Entry(value=value, expires_at=expires_at)\n\n    def delete(self, key: K) -> bool:\n        \"\"\"\n        Remove *key* from the cache.\n\n        Returns ``True`` if the key existed (and was removed), ``False`` otherwise.\n        \"\"\"\n        with self._lock:\n            if key in self._cache:\n                del self._cache[key]\n                return True\n            return False\n\n    def clear(self) -> None:\n        \"\"\"Remove all entries from the cache (statistics are preserved).\"\"\"\n        with self._lock:\n            self._cache.clear()\n\n    def contains(self, key: K) -> bool:\n        \"\"\"\n        Return ``True`` if *key* is present and not expired.\n\n        Does **not** update LRU order or statistics.\n        \"\"\"\n        with self._lock:\n            entry = self._cache.get(key)\n            if entry is None:\n                return False\n            if entry.is_expired():\n                del self._cache[key]\n                self._expired += 1\n                return False\n            return True\n\n    def stats(self) -> CacheStats:\n        \"\"\"Return a snapshot of the current hit/miss/eviction/expired counters.\"\"\"\n        with self._lock:\n            return CacheStats(\n                hits=self._hits,\n                misses=self._misses,\n                evictions=self._evictions,\n                expired=self._expired,\n            )\n\n    def reset_stats(self) -> None:\n        \"\"\"Reset all statistics counters to zero.\"\"\"\n        with self._lock:\n            self._hits = 0\n            self._misses = 0\n            self._evictions = 0\n            self._expired = 0\n\n    def size(self) -> int:\n        \"\"\"Return the number of entries currently in the cache (including potentially expired ones).\"\"\"\n        with self._lock:\n            return len(self._cache)\n\n    @property\n    def capacity(self) -> int:\n        \"\"\"Maximum number of entries this cache can hold.\"\"\"\n        return self._capacity\n\n    # ------------------------------------------------------------------\n    # Async API (thin wrappers, run sync core in executor)\n    # ------------------------------------------------------------------\n\n    async def aget(self, key: K, default: Optional[V] = None) -> Optional[V]:\n        \"\"\"\n        Async version of get().\n\n        Runs the synchronous implementation in the default thread-pool\n        executor so the event loop is not blocked.\n        \"\"\"\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(None, self.get, key, default)\n\n    async def aset(self, key: K, value: V, ttl: Optional[float] = None) -> None:\n        \"\"\"\n        Async version of set().\n\n        Runs the synchronous implementation in the default thread-pool\n        executor so the event loop is not blocked.\n        \"\"\"\n        loop = asyncio.get_running_loop()\n        await loop.run_in_executor(None, lambda: self.set(key, value, ttl))\n\n    # ------------------------------------------------------------------\n    # Dunder helpers\n    # ------------------------------------------------------------------\n\n    def __len__(self) -> int:\n        return self.size()\n\n    def __contains__(self, key: object) -> bool:\n        return self.contains(key)  # type: ignore[arg-type]\n\n    def __repr__(self) -> str:\n        return (\n            f\"LRUCache(capacity={self._capacity}, size={self.size()}, \"\n            f\"default_ttl={self._default_ttl})\"\n        )\n\n\n# ---------------------------------------------------------------------------\n# Usage example\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    print(\"=== Synchronous demo ===\")\n    cache: LRUCache[str, int] = LRUCache(capacity=3, default_ttl=5.0)\n\n    cache.set(\"a\", 1)\n    cache.set(\"b\", 2)\n    cache.set(\"c\", 3)\n    print(f\"get a={cache.get('a')}\")   # hit\n    print(f\"get d={cache.get('d')}\")   # miss\n\n    # Eviction: capacity=3, adding 'd' evicts LRU key ('b' since 'a' was just accessed)\n    cache.set(\"d\", 4)\n    print(f\"get b={cache.get('b')}\")   # miss (evicted)\n    print(f\"get c={cache.get('c')}\")   # hit\n    print(f\"get d={cache.get('d')}\")   # hit\n\n    print(cache.stats())\n    print(cache)\n\n    print(\"\\n=== TTL expiry demo ===\")\n    short_cache: LRUCache[str, str] = LRUCache(capacity=10)\n    short_cache.set(\"temp\", \"hello\", ttl=0.05)  # 50 ms TTL\n    print(f\"before expiry: {short_cache.get('temp')}\")\n    time.sleep(0.1)\n    print(f\"after expiry:  {short_cache.get('temp')}\")\n    print(short_cache.stats())\n\n    print(\"\\n=== Async demo ===\")\n\n    async def async_demo() -> None:\n        acache: LRUCache[str, str] = LRUCache(capacity=5, default_ttl=10.0)\n        await acache.aset(\"key1\", \"value1\")\n        await acache.aset(\"key2\", \"value2\")\n        v = await acache.aget(\"key1\")\n        print(f\"async get key1={v}\")\n        print(acache.stats())\n\n    asyncio.run(async_demo())\n",
  "final_code": "\"\"\"\nThread-safe LRU Cache with TTL Expiry and Async Support.\n\nProvides a doubly-linked list + hash map LRU eviction policy,\nper-entry TTL expiration, thread safety via RLock, async-compatible\nget/set methods (async wrappers around the sync core), and\nhit/miss statistics tracking.\n\"\"\"\n\nimport asyncio\nimport threading\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Any, Generic, Hashable, Optional, TypeVar\n\nK = TypeVar(\"K\", bound=Hashable)\nV = TypeVar(\"V\")\n\n\n@dataclass\nclass CacheStats:\n    \"\"\"Snapshot of cache hit/miss counters.\"\"\"\n    hits: int = 0\n    misses: int = 0\n    evictions: int = 0\n    expired: int = 0\n\n    @property\n    def total(self) -> int:\n        return self.hits + self.misses\n\n    @property\n    def hit_rate(self) -> float:\n        \"\"\"Fraction of lookups that were cache hits (0.0 if no lookups).\"\"\"\n        return self.hits / self.total if self.total > 0 else 0.0\n\n    def __repr__(self) -> str:\n        return (\n            f\"CacheStats(hits={self.hits}, misses={self.misses}, \"\n            f\"evictions={self.evictions}, expired={self.expired}, \"\n            f\"hit_rate={self.hit_rate:.2%})\"\n        )\n\n\n@dataclass\nclass _Entry:\n    \"\"\"Internal cache entry storing value, expiry timestamp, and access order.\"\"\"\n    value: Any\n    # Absolute expiry time from time.monotonic(); None means no expiry.\n    expires_at: Optional[float]\n\n    def is_expired(self) -> bool:\n        if self.expires_at is None:\n            return False\n        return time.monotonic() >= self.expires_at\n\n\nclass LRUCache(Generic[K, V]):\n    \"\"\"\n    Thread-safe Least-Recently-Used (LRU) cache with optional per-entry TTL.\n\n    Parameters\n    ----------\n    capacity : int\n        Maximum number of entries. Must be >= 1.\n    default_ttl : float | None\n        Default time-to-live in seconds for entries that do not specify one.\n        ``None`` (default) means entries never expire unless explicitly set.\n\n    Thread safety\n    -------------\n    All public synchronous methods are protected by a reentrant lock so the\n    cache can be shared across threads without external synchronisation.\n\n    Async support\n    -------------\n    ``aget`` and ``aset`` are thin async wrappers that call the synchronous\n    core on the running event loop's thread-pool executor, making them safe\n    to ``await`` from async code without blocking the event loop.\n\n    Examples\n    --------\n    >>> cache: LRUCache[str, int] = LRUCache(capacity=3, default_ttl=60)\n    >>> cache.set(\"a\", 1)\n    >>> cache.get(\"a\")\n    1\n    >>> cache.get(\"missing\")  # returns None\n    >>> cache.stats()\n    CacheStats(hits=1, misses=1, ...)\n    \"\"\"\n\n    def __init__(self, capacity: int, default_ttl: Optional[float] = None) -> None:\n        if capacity < 1:\n            raise ValueError(f\"capacity must be >= 1, got {capacity}\")\n        if default_ttl is not None and default_ttl <= 0:\n            raise ValueError(f\"default_ttl must be > 0, got {default_ttl}\")\n\n        self._capacity = capacity\n        self._default_ttl = default_ttl\n        # OrderedDict preserves insertion/move order; last = most recently used.\n        self._cache: OrderedDict[K, _Entry] = OrderedDict()\n        self._lock = threading.RLock()\n\n        # Statistics counters (protected by the same lock)\n        self._hits = 0\n        self._misses = 0\n        self._evictions = 0\n        self._expired = 0\n\n    # ------------------------------------------------------------------\n    # Public synchronous API\n    # ------------------------------------------------------------------\n\n    def get(self, key: K, default: Optional[V] = None) -> Optional[V]:\n        \"\"\"\n        Retrieve *key* from the cache.\n\n        Returns *default* (``None`` by default) on cache miss or if the\n        entry has expired. Expired entries are lazily removed on access.\n\n        Updates the LRU order on hit so *key* becomes the most-recently used.\n        \"\"\"\n        with self._lock:\n            entry = self._cache.get(key)\n            if entry is None:\n                self._misses += 1\n                return default\n\n            if entry.is_expired():\n                # Lazy expiry: remove stale entry and count as miss.\n                del self._cache[key]\n                self._expired += 1\n                self._misses += 1\n                return default\n\n            # Move to end (most recently used position).\n            self._cache.move_to_end(key)\n            self._hits += 1\n            return entry.value  # type: ignore[return-value]\n\n    def set(self, key: K, value: V, ttl: Optional[float] = None) -> None:\n        \"\"\"\n        Insert or update *key* -> *value*.\n\n        Parameters\n        ----------\n        key : K\n            Cache key (must be hashable).\n        value : V\n            Value to store.\n        ttl : float | None\n            Per-entry TTL in seconds. Overrides ``default_ttl`` when given.\n            ``None`` falls back to ``default_ttl``; if that is also ``None``\n            the entry never expires.  A non-positive TTL means the entry is\n            already expired; inserting such a new key into a full cache is a\n            no-op (no valid entry is evicted to make room for a dead-on-arrival\n            entry).\n        \"\"\"\n        with self._lock:\n            effective_ttl = ttl if ttl is not None else self._default_ttl\n            if effective_ttl is not None:\n                expires_at = time.monotonic() + effective_ttl\n            else:\n                expires_at = None\n\n            if key not in self._cache:\n                # For a new key we may need to make room first.\n                if len(self._cache) >= self._capacity:\n                    # If the new entry is already expired, don't evict anything.\n                    if expires_at is not None and time.monotonic() >= expires_at:\n                        return\n\n                    # Prefer evicting an already-expired entry over a valid LRU one.\n                    expired_key = next(\n                        (k for k, e in self._cache.items() if e.is_expired()), None\n                    )\n                    if expired_key is not None:\n                        del self._cache[expired_key]\n                        self._expired += 1\n                    else:\n                        self._cache.popitem(last=False)  # Remove oldest (LRU)\n                        self._evictions += 1\n\n            # Single assignment covers both insert and update; move_to_end\n            # promotes the key to MRU position in both cases.\n            self._cache[key] = _Entry(value=value, expires_at=expires_at)\n            self._cache.move_to_end(key)\n\n    def delete(self, key: K) -> bool:\n        \"\"\"\n        Remove *key* from the cache.\n\n        Returns ``True`` if the key existed (and was removed), ``False`` otherwise.\n        \"\"\"\n        with self._lock:\n            if key in self._cache:\n                del self._cache[key]\n                return True\n            return False\n\n    def clear(self) -> None:\n        \"\"\"Remove all entries from the cache (statistics are preserved).\"\"\"\n        with self._lock:\n            self._cache.clear()\n\n    def contains(self, key: K) -> bool:\n        \"\"\"\n        Return ``True`` if *key* is present and not expired.\n\n        This is a **pure read-only** operation: it does not update LRU order,\n        modify statistics, or remove expired entries. Use ``get()`` when you\n        also want the value and full LRU/stats semantics.\n        \"\"\"\n        with self._lock:\n            entry = self._cache.get(key)\n            if entry is None:\n                return False\n            return not entry.is_expired()\n\n    def stats(self) -> CacheStats:\n        \"\"\"Return a snapshot of the current hit/miss/eviction/expired counters.\"\"\"\n        with self._lock:\n            return CacheStats(\n                hits=self._hits,\n                misses=self._misses,\n                evictions=self._evictions,\n                expired=self._expired,\n            )\n\n    def reset_stats(self) -> None:\n        \"\"\"Reset all statistics counters to zero.\"\"\"\n        with self._lock:\n            self._hits = 0\n            self._misses = 0\n            self._evictions = 0\n            self._expired = 0\n\n    def size(self) -> int:\n        \"\"\"Return the number of entries currently in the cache.\n\n        Note: Because expired entries are removed lazily (only when accessed),\n        this count may include entries that are no longer valid. It reflects\n        the current memory footprint, not necessarily the number of usable items.\n        \"\"\"\n        with self._lock:\n            return len(self._cache)\n\n    @property\n    def capacity(self) -> int:\n        \"\"\"Maximum number of entries this cache can hold.\"\"\"\n        return self._capacity\n\n    # ------------------------------------------------------------------\n    # Async API (thin wrappers, run sync core in executor)\n    # ------------------------------------------------------------------\n\n    async def aget(self, key: K, default: Optional[V] = None) -> Optional[V]:\n        \"\"\"\n        Async version of get().\n\n        Runs the synchronous implementation in the default thread-pool\n        executor so the event loop is not blocked.\n        \"\"\"\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(None, self.get, key, default)\n\n    async def aset(self, key: K, value: V, ttl: Optional[float] = None) -> None:\n        \"\"\"\n        Async version of set().\n\n        Runs the synchronous implementation in the default thread-pool\n        executor so the event loop is not blocked.\n        \"\"\"\n        loop = asyncio.get_running_loop()\n        await loop.run_in_executor(None, lambda: self.set(key, value, ttl))\n\n    # ------------------------------------------------------------------\n    # Dunder helpers\n    # ------------------------------------------------------------------\n\n    def __len__(self) -> int:\n        return self.size()\n\n    def __contains__(self, key: object) -> bool:\n        return self.contains(key)  # type: ignore[arg-type]\n\n    def __repr__(self) -> str:\n        return (\n            f\"LRUCache(capacity={self._capacity}, size={self.size()}, \"\n            f\"default_ttl={self._default_ttl})\"\n        )\n\n\n# ---------------------------------------------------------------------------\n# Usage example\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    print(\"=== Synchronous demo ===\")\n    cache: LRUCache[str, int] = LRUCache(capacity=3, default_ttl=5.0)\n\n    cache.set(\"a\", 1)\n    cache.set(\"b\", 2)\n    cache.set(\"c\", 3)\n    print(f\"get a={cache.get('a')}\")   # hit\n    print(f\"get d={cache.get('d')}\")   # miss\n\n    # Eviction: capacity=3, adding 'd' evicts LRU key ('b' since 'a' was just accessed)\n    cache.set(\"d\", 4)\n    print(f\"get b={cache.get('b')}\")   # miss (evicted)\n    print(f\"get c={cache.get('c')}\")   # hit\n    print(f\"get d={cache.get('d')}\")   # hit\n\n    print(cache.stats())\n    print(cache)\n\n    print(\"\\n=== TTL expiry demo ===\")\n    short_cache: LRUCache[str, str] = LRUCache(capacity=10)\n    short_cache.set(\"temp\", \"hello\", ttl=0.05)  # 50 ms TTL\n    print(f\"before expiry: {short_cache.get('temp')}\")\n    time.sleep(0.1)\n    print(f\"after expiry:  {short_cache.get('temp')}\")\n    print(short_cache.stats())\n\n    print(\"\\n=== Async demo ===\")\n\n    async def async_demo() -> None:\n        acache: LRUCache[str, str] = LRUCache(capacity=5, default_ttl=10.0)\n        await acache.aset(\"key1\", \"value1\")\n        await acache.aset(\"key2\", \"value2\")\n        v = await acache.aget(\"key1\")\n        print(f\"async get key1={v}\")\n        print(acache.stats())\n\n    asyncio.run(async_demo())\n",
  "iterations": [
    {
      "number": 1,
      "review": "I couldn\u2019t write `reviews/review_1.md` because this session is read-only. Here is the full review content:\n\n## 1. TEST RESULTS\n\nRequested command:\n`python -m pytest tests.py -v --tb=long 2>&1 | tee test_run_reviewer.txt`\n\nResult in this environment:\nThe exact command failed due sandbox restrictions (`tee` write denied) and pytest temp-dir creation failure.\n\nFallback command used to still verify behavior:\n`python -m pytest tests.py -v --tb=long --capture=no -p no:cacheprovider`\n\nSummary:\n`60 passed, 0 failed` in ~1.00s.\n\n## 2. BUGS & CORRECTNESS\n\n1. Expired entries can cause eviction of valid entries (`solution.py:171`).\n`set()` evicts strictly by LRU when full, without first removing expired entries. This can evict a live entry while an expired entry remains in cache, which breaks expected TTL+LRU semantics.\n\n2. `contains()` behavior contradicts its own docstring (`solution.py:198`, `solution.py:206`).\nDoc says it does not update statistics, but it increments `expired` when it encounters stale entries.\n\n3. Test gap in async coverage (`tests.py:348`).\n`test_async_get_miss_returns_none` defines async `_run()` but never executes it with `asyncio.run(_run())`, so that test always passes without asserting behavior.\n\n## 3. SECURITY\n\nNo material security issues found.\nNo unsafe dynamic execution (`eval`/`exec`), no credential handling, and no injection surface in this scope.\n\n## 4. PERFORMANCE\n\n1. Missing expired-key cleanup on `set()` (`solution.py:171`) reduces effective capacity and can increase churn/evictions under TTL-heavy workloads.\n2. `aget`/`aset` always dispatch to threadpool (`solution.py:249`, `solution.py:260`), which is correct for loop compatibility but adds overhead for high-frequency async use.\n3. Minor overhead: `aset` allocates a lambda on each call (`solution.py:260`).\n\n## 5. CODE QUALITY\n\n1. Unused import: `field` is never used (`solution.py:14`).\n2. Inaccurate comment: `expires_at` says \u201csince epoch\u201d but code uses `time.monotonic()` (`solution.py:50`, `solution.py:56`, `solution.py:162`).\n3. Overall structure is clean and readable: good lock discipline, clear API, and strong test breadth.\n\n## 6. OVERALL VERDICT\n\n`GOOD`\n\nTop 3 MUST-fix:\n1. Purge expired entries before LRU eviction in `set()` (`solution.py:171`).\n2. Align `contains()` docs/behavior on stats mutation (`solution.py:198`, `solution.py:206`).\n3. Fix no-op async test (`tests.py:348`).\n\nNice-to-have improvements:\n1. Remove unused import (`solution.py:14`).\n2. Correct monotonic vs epoch comment (`solution.py:50`).\n3. Replace `lambda` in `aset` with `functools.partial` (`solution.py:260`).",
      "critique": "Here is a critical evaluation of the code review.\n\n## 1. MISSED ISSUES\n\nThe reviewer did a good job of finding the most critical flaws, but missed one subtle design inconsistency:\n\n*   **Inconsistent LRU Handling on Access:** The `get()` method correctly promotes a key to the most-recently-used position upon access, which is fundamental to an LRU cache. However, the `contains()` method (and by extension, the `in` operator) fails to do this. A `contains()` check is a form of access, and users of the cache would likely expect that checking for a key's existence would mark it as \"used\". This inconsistency can lead to surprising behavior where an item is evicted shortly after being checked with `if key in cache:`.\n\n## 2. FALSE POSITIVES\n\nThe review is largely accurate, but one point overstates its case:\n\n*   **Performance of `aget`/`aset`:** The review flags that `aget`/`aset` \"always dispatch to threadpool\" and this \"adds overhead\". While technically true, this is not a *problem*\u2014it is the *correct and intended design*. The purpose of these methods is to provide a non-blocking async interface to a fundamentally synchronous, lock-based library. Dispatching to an executor is the standard and proper pattern for this. Framing this as a performance issue is misleading; it's simply the trade-off inherent in bridging the sync/async gap, and the code implements it correctly.\n\n## 3. PRIORITY CALIBRATION\n\nThe reviewer's prioritization is generally excellent, but with room for refinement.\n\n*   **\"Must Fix\" items:**\n    1.  `set()` eviction logic: **Correctly critical.** This is a correctness bug.\n    2.  `contains()` docstring/behavior: **Correctly critical.** The reviewer identifies a docstring conflict, but the underlying issue is a design flaw (a method with surprising side-effects). This is a high-priority issue.\n    3.  Fix no-op async test: **Correctly critical.** Broken tests undermine the entire validation process.\n*   **\"Nice to have\" items:** All items (unused import, comment correction, `aset` lambda) are correctly identified as minor and are well-prioritized below the \"Must Fix\" list.\n\n**Conclusion:** The priorities are sound. The reviewer correctly separated critical bugs from minor quality-of-life improvements.\n\n## 4. BALANCE ASSESSMENT\n\nThe review is **about right**.\n\nIt is professional, evidence-based, and strikes a good balance between identifying significant faults and acknowledging the code's strengths (\"clean and readable\", \"strong test breadth\"). The \"GOOD\" verdict is fair: the code has a solid foundation but is undermined by a few critical bugs that prevent it from being \"Excellent\". The feedback is constructive and provides a clear, actionable path for improvement without being harsh or nitpicky.\n\n## 5. ACTIONABLE RECOMMENDATIONS\n\nHere are the top 5 priorities for the code author, ranked by importance. This list synthesizes the valid points from the review and the analysis above.\n\n1.  **Fix the `set()` Eviction Bug.** This is the most severe issue. When the cache is at capacity, `set()` must prioritize removing expired entries over evicting the least-recently-used *valid* entry. The current logic can cause premature eviction of good data.\n\n2.  **Redesign `contains()` Semantics.** The current `contains()` method has surprising side effects (deleting items, updating stats) and is inconsistent with `get()` (it doesn't update the LRU order). Decide on a clear semantic and stick to it. **Recommendation:** Make `contains()` a pure, read-only operation with no side effects (no deletion, no stat updates, no LRU promotion). This follows the principle of least surprise. Update the docstring accordingly.\n\n3.  **Fix the Inoperative Async Test.** The `test_async_get_miss_returns_none` test must be fixed to actually execute its async function and assert the behavior. A non-running test creates a dangerous blind spot in test coverage.\n\n4.  **Correct All Documentation and Comments.** Once the code's behavior is fixed, ensure all comments and docstrings are 100% accurate. This includes the `expires_at` comment (monotonic vs. epoch) and the `contains()` docstring. For a reusable class, documentation is part of the API.\n\n5.  **Remove the Unused `field` Import.** While minor, this is a quick fix that improves code hygiene.",
      "revision": "\"\"\"\nThread-safe LRU Cache with TTL Expiry and Async Support.\n\nProvides a doubly-linked list + hash map LRU eviction policy,\nper-entry TTL expiration, thread safety via RLock, async-compatible\nget/set methods (async wrappers around the sync core), and\nhit/miss statistics tracking.\n\"\"\"\n\nimport asyncio\nimport threading\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Any, Generic, Hashable, Optional, TypeVar\n\nK = TypeVar(\"K\", bound=Hashable)\nV = TypeVar(\"V\")\n\n\n@dataclass\nclass CacheStats:\n    \"\"\"Snapshot of cache hit/miss counters.\"\"\"\n    hits: int = 0\n    misses: int = 0\n    evictions: int = 0\n    expired: int = 0\n\n    @property\n    def total(self) -> int:\n        return self.hits + self.misses\n\n    @property\n    def hit_rate(self) -> float:\n        \"\"\"Fraction of lookups that were cache hits (0.0 if no lookups).\"\"\"\n        return self.hits / self.total if self.total > 0 else 0.0\n\n    def __repr__(self) -> str:\n        return (\n            f\"CacheStats(hits={self.hits}, misses={self.misses}, \"\n            f\"evictions={self.evictions}, expired={self.expired}, \"\n            f\"hit_rate={self.hit_rate:.2%})\"\n        )\n\n\n@dataclass\nclass _Entry:\n    \"\"\"Internal cache entry storing value, expiry timestamp, and access order.\"\"\"\n    value: Any\n    # Absolute expiry time from time.monotonic(); None means no expiry.\n    expires_at: Optional[float]\n\n    def is_expired(self) -> bool:\n        if self.expires_at is None:\n            return False\n        return time.monotonic() >= self.expires_at\n\n\nclass LRUCache(Generic[K, V]):\n    \"\"\"\n    Thread-safe Least-Recently-Used (LRU) cache with optional per-entry TTL.\n\n    Parameters\n    ----------\n    capacity : int\n        Maximum number of entries. Must be >= 1.\n    default_ttl : float | None\n        Default time-to-live in seconds for entries that do not specify one.\n        ``None`` (default) means entries never expire unless explicitly set.\n\n    Thread safety\n    -------------\n    All public synchronous methods are protected by a reentrant lock so the\n    cache can be shared across threads without external synchronisation.\n\n    Async support\n    -------------\n    ``aget`` and ``aset`` are thin async wrappers that call the synchronous\n    core on the running event loop's thread-pool executor, making them safe\n    to ``await`` from async code without blocking the event loop.\n\n    Examples\n    --------\n    >>> cache: LRUCache[str, int] = LRUCache(capacity=3, default_ttl=60)\n    >>> cache.set(\"a\", 1)\n    >>> cache.get(\"a\")\n    1\n    >>> cache.get(\"missing\")  # returns None\n    >>> cache.stats()\n    CacheStats(hits=1, misses=1, ...)\n    \"\"\"\n\n    def __init__(self, capacity: int, default_ttl: Optional[float] = None) -> None:\n        if capacity < 1:\n            raise ValueError(f\"capacity must be >= 1, got {capacity}\")\n        if default_ttl is not None and default_ttl <= 0:\n            raise ValueError(f\"default_ttl must be > 0, got {default_ttl}\")\n\n        self._capacity = capacity\n        self._default_ttl = default_ttl\n        # OrderedDict preserves insertion/move order; last = most recently used.\n        self._cache: OrderedDict[K, _Entry] = OrderedDict()\n        self._lock = threading.RLock()\n\n        # Statistics counters (protected by the same lock)\n        self._hits = 0\n        self._misses = 0\n        self._evictions = 0\n        self._expired = 0\n\n    # ------------------------------------------------------------------\n    # Public synchronous API\n    # ------------------------------------------------------------------\n\n    def get(self, key: K, default: Optional[V] = None) -> Optional[V]:\n        \"\"\"\n        Retrieve *key* from the cache.\n\n        Returns *default* (``None`` by default) on cache miss or if the\n        entry has expired. Expired entries are lazily removed on access.\n\n        Updates the LRU order on hit so *key* becomes the most-recently used.\n        \"\"\"\n        with self._lock:\n            entry = self._cache.get(key)\n            if entry is None:\n                self._misses += 1\n                return default\n\n            if entry.is_expired():\n                # Lazy expiry: remove stale entry and count as miss.\n                del self._cache[key]\n                self._expired += 1\n                self._misses += 1\n                return default\n\n            # Move to end (most recently used position).\n            self._cache.move_to_end(key)\n            self._hits += 1\n            return entry.value  # type: ignore[return-value]\n\n    def set(self, key: K, value: V, ttl: Optional[float] = None) -> None:\n        \"\"\"\n        Insert or update *key* -> *value*.\n\n        Parameters\n        ----------\n        key : K\n            Cache key (must be hashable).\n        value : V\n            Value to store.\n        ttl : float | None\n            Per-entry TTL in seconds. Overrides ``default_ttl`` when given.\n            Pass ``0`` or a negative number to get an immediate expiry\n            (the entry will be expired on the next access).\n            ``None`` falls back to ``default_ttl``; if that is also ``None``\n            the entry never expires.\n        \"\"\"\n        with self._lock:\n            effective_ttl = ttl if ttl is not None else self._default_ttl\n            if effective_ttl is not None:\n                expires_at = time.monotonic() + effective_ttl\n            else:\n                expires_at = None\n\n            if key in self._cache:\n                # Update existing entry and promote to MRU position.\n                self._cache[key] = _Entry(value=value, expires_at=expires_at)\n                self._cache.move_to_end(key)\n            else:\n                # Evict an expired entry first (if any) before falling back to LRU.\n                if len(self._cache) >= self._capacity:\n                    expired_key = next(\n                        (k for k, e in self._cache.items() if e.is_expired()), None\n                    )\n                    if expired_key is not None:\n                        del self._cache[expired_key]\n                        self._expired += 1\n                    else:\n                        self._cache.popitem(last=False)  # Remove oldest (LRU)\n                        self._evictions += 1\n                self._cache[key] = _Entry(value=value, expires_at=expires_at)\n\n    def delete(self, key: K) -> bool:\n        \"\"\"\n        Remove *key* from the cache.\n\n        Returns ``True`` if the key existed (and was removed), ``False`` otherwise.\n        \"\"\"\n        with self._lock:\n            if key in self._cache:\n                del self._cache[key]\n                return True\n            return False\n\n    def clear(self) -> None:\n        \"\"\"Remove all entries from the cache (statistics are preserved).\"\"\"\n        with self._lock:\n            self._cache.clear()\n\n    def contains(self, key: K) -> bool:\n        \"\"\"\n        Return ``True`` if *key* is present and not expired.\n\n        This is a **pure read-only** operation: it does not update LRU order,\n        modify statistics, or remove expired entries. Use ``get()`` when you\n        also want the value and full LRU/stats semantics.\n        \"\"\"\n        with self._lock:\n            entry = self._cache.get(key)\n            if entry is None:\n                return False\n            return not entry.is_expired()\n\n    def stats(self) -> CacheStats:\n        \"\"\"Return a snapshot of the current hit/miss/eviction/expired counters.\"\"\"\n        with self._lock:\n            return CacheStats(\n                hits=self._hits,\n                misses=self._misses,\n                evictions=self._evictions,\n                expired=self._expired,\n            )\n\n    def reset_stats(self) -> None:\n        \"\"\"Reset all statistics counters to zero.\"\"\"\n        with self._lock:\n            self._hits = 0\n            self._misses = 0\n            self._evictions = 0\n            self._expired = 0\n\n    def size(self) -> int:\n        \"\"\"Return the number of entries currently in the cache (including potentially expired ones).\"\"\"\n        with self._lock:\n            return len(self._cache)\n\n    @property\n    def capacity(self) -> int:\n        \"\"\"Maximum number of entries this cache can hold.\"\"\"\n        return self._capacity\n\n    # ------------------------------------------------------------------\n    # Async API (thin wrappers, run sync core in executor)\n    # ------------------------------------------------------------------\n\n    async def aget(self, key: K, default: Optional[V] = None) -> Optional[V]:\n        \"\"\"\n        Async version of get().\n\n        Runs the synchronous implementation in the default thread-pool\n        executor so the event loop is not blocked.\n        \"\"\"\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(None, self.get, key, default)\n\n    async def aset(self, key: K, value: V, ttl: Optional[float] = None) -> None:\n        \"\"\"\n        Async version of set().\n\n        Runs the synchronous implementation in the default thread-pool\n        executor so the event loop is not blocked.\n        \"\"\"\n        loop = asyncio.get_running_loop()\n        await loop.run_in_executor(None, lambda: self.set(key, value, ttl))\n\n    # ------------------------------------------------------------------\n    # Dunder helpers\n    # ------------------------------------------------------------------\n\n    def __len__(self) -> int:\n        return self.size()\n\n    def __contains__(self, key: object) -> bool:\n        return self.contains(key)  # type: ignore[arg-type]\n\n    def __repr__(self) -> str:\n        return (\n            f\"LRUCache(capacity={self._capacity}, size={self.size()}, \"\n            f\"default_ttl={self._default_ttl})\"\n        )\n\n\n# ---------------------------------------------------------------------------\n# Usage example\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    print(\"=== Synchronous demo ===\")\n    cache: LRUCache[str, int] = LRUCache(capacity=3, default_ttl=5.0)\n\n    cache.set(\"a\", 1)\n    cache.set(\"b\", 2)\n    cache.set(\"c\", 3)\n    print(f\"get a={cache.get('a')}\")   # hit\n    print(f\"get d={cache.get('d')}\")   # miss\n\n    # Eviction: capacity=3, adding 'd' evicts LRU key ('b' since 'a' was just accessed)\n    cache.set(\"d\", 4)\n    print(f\"get b={cache.get('b')}\")   # miss (evicted)\n    print(f\"get c={cache.get('c')}\")   # hit\n    print(f\"get d={cache.get('d')}\")   # hit\n\n    print(cache.stats())\n    print(cache)\n\n    print(\"\\n=== TTL expiry demo ===\")\n    short_cache: LRUCache[str, str] = LRUCache(capacity=10)\n    short_cache.set(\"temp\", \"hello\", ttl=0.05)  # 50 ms TTL\n    print(f\"before expiry: {short_cache.get('temp')}\")\n    time.sleep(0.1)\n    print(f\"after expiry:  {short_cache.get('temp')}\")\n    print(short_cache.stats())\n\n    print(\"\\n=== Async demo ===\")\n\n    async def async_demo() -> None:\n        acache: LRUCache[str, str] = LRUCache(capacity=5, default_ttl=10.0)\n        await acache.aset(\"key1\", \"value1\")\n        await acache.aset(\"key2\", \"value2\")\n        v = await acache.aget(\"key1\")\n        print(f\"async get key1={v}\")\n        print(acache.stats())\n\n    asyncio.run(async_demo())\n"
    },
    {
      "number": 2,
      "review": "I could not write `reviews/review_2.md` because the workspace is read-only (`operation not permitted` on file create).  \nBelow is the exact review content to place in `reviews/review_2.md`:\n\n```md\n## 1. TEST RESULTS\nRequested command could not run as-is in this sandbox:\n- `python -m pytest tests.py -v --tb=long 2>&1 | tee test_run_reviewer.txt`\n- Failure: `tee: ... Operation not permitted` and pytest tempdir init error (`No usable temporary directory found`).\n\nWrite-free fallback run succeeded:\n- `PYTHONDONTWRITEBYTECODE=1 PYTEST_DISABLE_PLUGIN_AUTOLOAD=1 python -m pytest tests.py -v --tb=long -s -p no:cacheprovider`\n\nSummary:\n- `collected 60 items`\n- `============================== 60 passed in 1.10s ==============================`\n\n## 2. CHANGES SINCE LAST REVIEW\n- Issues resolved: expired-first eviction logic in `set()` was added (`solution.py:171`); `contains()` semantics/doc now align as pure read-only (`solution.py:205`); async miss test now actually executes (`tests.py:348`); monotonic-time comment fixed (`solution.py:50`); unused `field` import removed.\n- Issues NOT addressed: `reviews/review_1.md` is missing from this workspace, so direct item-by-item reconciliation against that file is not possible.\n- New issues introduced by the revision: none observed.\n\n## 3. BUGS & CORRECTNESS (Updated)\n- MEDIUM: inserting a new key with `ttl <= 0` at full capacity can evict a valid entry to make room for an already-expired one (`solution.py:160`, `solution.py:172`, `solution.py:182`). This causes avoidable data loss/churn for a value that is dead on arrival.\n- LOW (testing gap): there is no test that covers the above edge case (`tests.py:529` currently uses `0.0001`, not `0`/negative, and does not test full-capacity behavior).\n\n## 4. SECURITY (Updated)\n- No direct security vulnerabilities identified.\n- No untrusted-input parsing, serialization, or external I/O surfaces are present in this implementation.\n\n## 5. PERFORMANCE (Updated)\n- `set()` does an O(n) scan for expired entries when full (`solution.py:173`), which is acceptable for moderate capacities but can become costly under high churn and large cache sizes.\n- Critique false-positive acknowledged: executor dispatch in `aget`/`aset` is expected for async compatibility with a lock-based sync core, not a design defect (`solution.py:255`, `solution.py:265`).\n\n## 6. CODE QUALITY (Updated)\n- Overall readability and API documentation are strong; thread-safety boundaries are clear.\n- The revision improved consistency between behavior and docs, especially around `contains()`.\n- Remaining quality gap is mostly edge-case test coverage (TTL<=0 with full cache).\n\n## 7. OVERALL VERDICT (Updated)\n- Critic false-positive reconsideration: agreed that `aget`/`aset` executor usage should not be treated as a bug; that prior concern is withdrawn.\n- Critic point on `contains()` semantics: current implementation intentionally adopts pure read-only semantics and documents that clearly, which is a coherent design choice.\n- Rating: GOOD\n- Remaining MUST-FIX issues: handle `ttl <= 0` insertion without evicting valid entries when at capacity (`solution.py:160` / `solution.py:172`).\n```",
      "critique": "Here is a critical evaluation of the code review.\n\n## 1. MISSED ISSUES\n\nThe reviewer correctly identified the most significant remaining bug, but missed a minor code quality and efficiency issue in the same `set()` method they were analyzing:\n\n*   **Inefficient `set()` Implementation:** The current logic for inserting/updating an entry uses an `if key in self._cache:` check. This is slightly inefficient as it can lead to multiple dictionary lookups for a single operation. A more idiomatic and efficient approach would be to perform the assignment (`self._cache[key] = ...`) and then call `self._cache.move_to_end(key)`. The assignment handles both insertion and update, and `move_to_end` correctly promotes the key in both cases (it's a no-op for a newly added key at the end and a reordering for an existing key). This would simplify the code and remove a redundant hash lookup on updates.\n\n## 2. FALSE POSITIVES\n\nThe review is highly accurate and contains **no false positives**.\n\n*   The primary bug identified (`ttl <= 0` insertion at full capacity causing eviction of a valid item) is a real, demonstrable problem with the current logic.\n*   The performance observation regarding the O(n) scan for expired items is also a correct and relevant analysis of the implementation's trade-offs.\n\n## 3. PRIORITY CALIBRATION\n\nThe reviewer's prioritization is sound, but the severity of the main bug could be stated more strongly.\n\n*   **\"MUST-FIX\" Item:** The reviewer is correct to label the `ttl <= 0` insertion bug as the highest priority. It represents a correctness flaw that causes avoidable data loss.\n*   **Severity Understatement:** The reviewer labels this bug \"MEDIUM\". It could be argued this is \"HIGH\". A cache's primary responsibility is to keep useful data. A logic path that evicts a valid, useful entry to make room for one that is known to be immediately useless is a significant violation of that responsibility, even if it is an edge case.\n*   **Overall Verdict:** The \"GOOD\" rating is fair. The code has a solid structure and the previous, more critical, flaws have been fixed. The remaining issue, while important, is confined to a specific edge case.\n\n## 4. BALANCE ASSESSMENT\n\nThe review is **about right**.\n\nIt is fair, fact-based, and constructive. It correctly acknowledges the improvements made since the last cycle, withdraws its own prior false positives, and focuses on the most important remaining issue. The tone is professional and collaborative.\n\n## 5. ACTIONABLE RECOMMENDATIONS\n\nHere are the top 4 priorities the code author should focus on next, synthesizing the reviewer's valid points with the analysis above.\n\n1.  **Fix the `ttl <= 0` Insertion Bug.** This remains the top priority. An incoming item with a non-positive TTL should not cause the eviction of a *valid* LRU item. The fix should be to short-circuit the insertion: if the item is new, the cache is full, no expired items can be cleared, and the new item's `ttl` makes it dead-on-arrival, the insertion should be skipped entirely.\n\n2.  **Add Specific Tests for the `ttl <= 0` Edge Case.** As the reviewer noted, a testing gap exists. A new test should be created that fills a cache to capacity, then attempts to set a new key with `ttl=0` and `ttl=-1`, asserting that a valid entry was *not* evicted and the cache size did not grow.\n\n3.  **Refactor `set()` for Efficiency.** Improve the implementation by removing the `if key in self._cache:` branch. A single path for assignment followed by an unconditional `move_to_end(key)` is cleaner, more efficient, and less complex.\n\n4.  **Clarify `size()` Behavior in Docstring.** The docstring for `size()` is technically correct but could be more helpful. Add a sentence to explain the implication of lazy expiry, for example: \"Note: Because expired entries are removed lazily, this count may include entries that are no longer valid. It reflects the current memory footprint, not necessarily the number of usable items.\"",
      "revision": "\"\"\"\nThread-safe LRU Cache with TTL Expiry and Async Support.\n\nProvides a doubly-linked list + hash map LRU eviction policy,\nper-entry TTL expiration, thread safety via RLock, async-compatible\nget/set methods (async wrappers around the sync core), and\nhit/miss statistics tracking.\n\"\"\"\n\nimport asyncio\nimport threading\nimport time\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Any, Generic, Hashable, Optional, TypeVar\n\nK = TypeVar(\"K\", bound=Hashable)\nV = TypeVar(\"V\")\n\n\n@dataclass\nclass CacheStats:\n    \"\"\"Snapshot of cache hit/miss counters.\"\"\"\n    hits: int = 0\n    misses: int = 0\n    evictions: int = 0\n    expired: int = 0\n\n    @property\n    def total(self) -> int:\n        return self.hits + self.misses\n\n    @property\n    def hit_rate(self) -> float:\n        \"\"\"Fraction of lookups that were cache hits (0.0 if no lookups).\"\"\"\n        return self.hits / self.total if self.total > 0 else 0.0\n\n    def __repr__(self) -> str:\n        return (\n            f\"CacheStats(hits={self.hits}, misses={self.misses}, \"\n            f\"evictions={self.evictions}, expired={self.expired}, \"\n            f\"hit_rate={self.hit_rate:.2%})\"\n        )\n\n\n@dataclass\nclass _Entry:\n    \"\"\"Internal cache entry storing value, expiry timestamp, and access order.\"\"\"\n    value: Any\n    # Absolute expiry time from time.monotonic(); None means no expiry.\n    expires_at: Optional[float]\n\n    def is_expired(self) -> bool:\n        if self.expires_at is None:\n            return False\n        return time.monotonic() >= self.expires_at\n\n\nclass LRUCache(Generic[K, V]):\n    \"\"\"\n    Thread-safe Least-Recently-Used (LRU) cache with optional per-entry TTL.\n\n    Parameters\n    ----------\n    capacity : int\n        Maximum number of entries. Must be >= 1.\n    default_ttl : float | None\n        Default time-to-live in seconds for entries that do not specify one.\n        ``None`` (default) means entries never expire unless explicitly set.\n\n    Thread safety\n    -------------\n    All public synchronous methods are protected by a reentrant lock so the\n    cache can be shared across threads without external synchronisation.\n\n    Async support\n    -------------\n    ``aget`` and ``aset`` are thin async wrappers that call the synchronous\n    core on the running event loop's thread-pool executor, making them safe\n    to ``await`` from async code without blocking the event loop.\n\n    Examples\n    --------\n    >>> cache: LRUCache[str, int] = LRUCache(capacity=3, default_ttl=60)\n    >>> cache.set(\"a\", 1)\n    >>> cache.get(\"a\")\n    1\n    >>> cache.get(\"missing\")  # returns None\n    >>> cache.stats()\n    CacheStats(hits=1, misses=1, ...)\n    \"\"\"\n\n    def __init__(self, capacity: int, default_ttl: Optional[float] = None) -> None:\n        if capacity < 1:\n            raise ValueError(f\"capacity must be >= 1, got {capacity}\")\n        if default_ttl is not None and default_ttl <= 0:\n            raise ValueError(f\"default_ttl must be > 0, got {default_ttl}\")\n\n        self._capacity = capacity\n        self._default_ttl = default_ttl\n        # OrderedDict preserves insertion/move order; last = most recently used.\n        self._cache: OrderedDict[K, _Entry] = OrderedDict()\n        self._lock = threading.RLock()\n\n        # Statistics counters (protected by the same lock)\n        self._hits = 0\n        self._misses = 0\n        self._evictions = 0\n        self._expired = 0\n\n    # ------------------------------------------------------------------\n    # Public synchronous API\n    # ------------------------------------------------------------------\n\n    def get(self, key: K, default: Optional[V] = None) -> Optional[V]:\n        \"\"\"\n        Retrieve *key* from the cache.\n\n        Returns *default* (``None`` by default) on cache miss or if the\n        entry has expired. Expired entries are lazily removed on access.\n\n        Updates the LRU order on hit so *key* becomes the most-recently used.\n        \"\"\"\n        with self._lock:\n            entry = self._cache.get(key)\n            if entry is None:\n                self._misses += 1\n                return default\n\n            if entry.is_expired():\n                # Lazy expiry: remove stale entry and count as miss.\n                del self._cache[key]\n                self._expired += 1\n                self._misses += 1\n                return default\n\n            # Move to end (most recently used position).\n            self._cache.move_to_end(key)\n            self._hits += 1\n            return entry.value  # type: ignore[return-value]\n\n    def set(self, key: K, value: V, ttl: Optional[float] = None) -> None:\n        \"\"\"\n        Insert or update *key* -> *value*.\n\n        Parameters\n        ----------\n        key : K\n            Cache key (must be hashable).\n        value : V\n            Value to store.\n        ttl : float | None\n            Per-entry TTL in seconds. Overrides ``default_ttl`` when given.\n            ``None`` falls back to ``default_ttl``; if that is also ``None``\n            the entry never expires.  A non-positive TTL means the entry is\n            already expired; inserting such a new key into a full cache is a\n            no-op (no valid entry is evicted to make room for a dead-on-arrival\n            entry).\n        \"\"\"\n        with self._lock:\n            effective_ttl = ttl if ttl is not None else self._default_ttl\n            if effective_ttl is not None:\n                expires_at = time.monotonic() + effective_ttl\n            else:\n                expires_at = None\n\n            if key not in self._cache:\n                # For a new key we may need to make room first.\n                if len(self._cache) >= self._capacity:\n                    # If the new entry is already expired, don't evict anything.\n                    if expires_at is not None and time.monotonic() >= expires_at:\n                        return\n\n                    # Prefer evicting an already-expired entry over a valid LRU one.\n                    expired_key = next(\n                        (k for k, e in self._cache.items() if e.is_expired()), None\n                    )\n                    if expired_key is not None:\n                        del self._cache[expired_key]\n                        self._expired += 1\n                    else:\n                        self._cache.popitem(last=False)  # Remove oldest (LRU)\n                        self._evictions += 1\n\n            # Single assignment covers both insert and update; move_to_end\n            # promotes the key to MRU position in both cases.\n            self._cache[key] = _Entry(value=value, expires_at=expires_at)\n            self._cache.move_to_end(key)\n\n    def delete(self, key: K) -> bool:\n        \"\"\"\n        Remove *key* from the cache.\n\n        Returns ``True`` if the key existed (and was removed), ``False`` otherwise.\n        \"\"\"\n        with self._lock:\n            if key in self._cache:\n                del self._cache[key]\n                return True\n            return False\n\n    def clear(self) -> None:\n        \"\"\"Remove all entries from the cache (statistics are preserved).\"\"\"\n        with self._lock:\n            self._cache.clear()\n\n    def contains(self, key: K) -> bool:\n        \"\"\"\n        Return ``True`` if *key* is present and not expired.\n\n        This is a **pure read-only** operation: it does not update LRU order,\n        modify statistics, or remove expired entries. Use ``get()`` when you\n        also want the value and full LRU/stats semantics.\n        \"\"\"\n        with self._lock:\n            entry = self._cache.get(key)\n            if entry is None:\n                return False\n            return not entry.is_expired()\n\n    def stats(self) -> CacheStats:\n        \"\"\"Return a snapshot of the current hit/miss/eviction/expired counters.\"\"\"\n        with self._lock:\n            return CacheStats(\n                hits=self._hits,\n                misses=self._misses,\n                evictions=self._evictions,\n                expired=self._expired,\n            )\n\n    def reset_stats(self) -> None:\n        \"\"\"Reset all statistics counters to zero.\"\"\"\n        with self._lock:\n            self._hits = 0\n            self._misses = 0\n            self._evictions = 0\n            self._expired = 0\n\n    def size(self) -> int:\n        \"\"\"Return the number of entries currently in the cache.\n\n        Note: Because expired entries are removed lazily (only when accessed),\n        this count may include entries that are no longer valid. It reflects\n        the current memory footprint, not necessarily the number of usable items.\n        \"\"\"\n        with self._lock:\n            return len(self._cache)\n\n    @property\n    def capacity(self) -> int:\n        \"\"\"Maximum number of entries this cache can hold.\"\"\"\n        return self._capacity\n\n    # ------------------------------------------------------------------\n    # Async API (thin wrappers, run sync core in executor)\n    # ------------------------------------------------------------------\n\n    async def aget(self, key: K, default: Optional[V] = None) -> Optional[V]:\n        \"\"\"\n        Async version of get().\n\n        Runs the synchronous implementation in the default thread-pool\n        executor so the event loop is not blocked.\n        \"\"\"\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(None, self.get, key, default)\n\n    async def aset(self, key: K, value: V, ttl: Optional[float] = None) -> None:\n        \"\"\"\n        Async version of set().\n\n        Runs the synchronous implementation in the default thread-pool\n        executor so the event loop is not blocked.\n        \"\"\"\n        loop = asyncio.get_running_loop()\n        await loop.run_in_executor(None, lambda: self.set(key, value, ttl))\n\n    # ------------------------------------------------------------------\n    # Dunder helpers\n    # ------------------------------------------------------------------\n\n    def __len__(self) -> int:\n        return self.size()\n\n    def __contains__(self, key: object) -> bool:\n        return self.contains(key)  # type: ignore[arg-type]\n\n    def __repr__(self) -> str:\n        return (\n            f\"LRUCache(capacity={self._capacity}, size={self.size()}, \"\n            f\"default_ttl={self._default_ttl})\"\n        )\n\n\n# ---------------------------------------------------------------------------\n# Usage example\n# ---------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    print(\"=== Synchronous demo ===\")\n    cache: LRUCache[str, int] = LRUCache(capacity=3, default_ttl=5.0)\n\n    cache.set(\"a\", 1)\n    cache.set(\"b\", 2)\n    cache.set(\"c\", 3)\n    print(f\"get a={cache.get('a')}\")   # hit\n    print(f\"get d={cache.get('d')}\")   # miss\n\n    # Eviction: capacity=3, adding 'd' evicts LRU key ('b' since 'a' was just accessed)\n    cache.set(\"d\", 4)\n    print(f\"get b={cache.get('b')}\")   # miss (evicted)\n    print(f\"get c={cache.get('c')}\")   # hit\n    print(f\"get d={cache.get('d')}\")   # hit\n\n    print(cache.stats())\n    print(cache)\n\n    print(\"\\n=== TTL expiry demo ===\")\n    short_cache: LRUCache[str, str] = LRUCache(capacity=10)\n    short_cache.set(\"temp\", \"hello\", ttl=0.05)  # 50 ms TTL\n    print(f\"before expiry: {short_cache.get('temp')}\")\n    time.sleep(0.1)\n    print(f\"after expiry:  {short_cache.get('temp')}\")\n    print(short_cache.stats())\n\n    print(\"\\n=== Async demo ===\")\n\n    async def async_demo() -> None:\n        acache: LRUCache[str, str] = LRUCache(capacity=5, default_ttl=10.0)\n        await acache.aset(\"key1\", \"value1\")\n        await acache.aset(\"key2\", \"value2\")\n        v = await acache.aget(\"key1\")\n        print(f\"async get key1={v}\")\n        print(acache.stats())\n\n    asyncio.run(async_demo())\n"
    }
  ]
}